{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP: Recommendations and Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Generate Recommendations from LDA Transformation\n",
    "\n",
    "In this part we will transform a set of product descriptions using TfIdf and LDA topic modeling to generate product recommendations based on similarity in LDA space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and transform text using TfIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   name_title   5000 non-null   object\n",
      " 1   description  5000 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 78.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Load the Data\n",
    "\n",
    "# The dataset we'll be working with is a set of product descriptions from the JCPenney department store.\n",
    "\n",
    "# Load product information from ../data/jcpenney-products_subset.csv.zip\n",
    "df_jcp = pd.read_csv('../data/jcpenney-products_subset.csv.zip')\n",
    "print(df_jcp.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invicta® Sl Rally Mens Black Leather Strap Chronograph Watch 16012\n",
      "--------------------------------------------------\n",
      "A timepiece you can enjoy every day of the week, this sports car-inspired chronograph watch packs plenty of information into an easy-to-read dial.   Brand: Invicta Dial Color: Black Strap: Black leather Clasp: Buckle Movement: Quartz Water Resistance: 100m Case Width: 48mm Case Thickness: 13.5mm Bracelet Dimensions: 210mm long; 22mm wide Model No.: 16012 Special Features: Stopwatch; 3 multifunction sub dials   Jewelry photos are enlarged to show detail.\n"
     ]
    }
   ],
   "source": [
    "# Print an Example\n",
    "\n",
    "print(df_jcp['name_title'][0])\n",
    "print('-'*50) \n",
    "print(df_jcp['description'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 5678)\n"
     ]
    }
   ],
   "source": [
    "# Transform Descriptions using TfIdf\n",
    "\n",
    "# In order to pass our product descriptions to the LDA model, we first\n",
    "#   need to vectorize from strings to fixed length vectors of floats.\n",
    "# To do this we will transform our documents into a TfIdf representation.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), min_df=10, max_df=0.1)\n",
    "X_tfidf = tfidf.fit_transform(df_jcp['description'])\n",
    "\n",
    "# Print the shape of X_tfidf \n",
    "print(X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['jewelry photos', 'features stopwatch', 'special features',\n",
      "       'model no', 'wide model', '22mm wide', 'long 22mm',\n",
      "       'bracelet dimensions', 'case thickness', 'case width',\n",
      "       'resistance 100m', 'water resistance', 'quartz water',\n",
      "       'movement quartz', 'buckle movement', 'clasp buckle',\n",
      "       'leather clasp', 'black leather', 'strap black', 'black strap',\n",
      "       'color black', 'dial color', 'to read', 'easy to', 'an easy',\n",
      "       'plenty of', 'of the', 'day of', 'every day', 'you can', 'sub',\n",
      "       'stopwatch', 'special', 'no', 'model', 'wide', '22mm',\n",
      "       'dimensions', 'bracelet', '5mm', '13', 'thickness', 'width',\n",
      "       'case', '100m', 'resistance', 'water', 'quartz', 'movement',\n",
      "       'buckle', 'clasp', 'leather', 'strap', 'black', 'color', 'brand',\n",
      "       'dial', 'read', 'into', 'plenty', 'watch', 'chronograph',\n",
      "       'inspired', 'car', 'sports', 'week', 'day', 'every', 'enjoy',\n",
      "       'can'], dtype='<U24')]\n"
     ]
    }
   ],
   "source": [
    "# Show The Terms Extracted From Row 0\n",
    "print(tfidf.inverse_transform(X_tfidf[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zipper_pocket', 'zipper_pockets', 'zippered', 'zirconia', 'zone']\n"
     ]
    }
   ],
   "source": [
    "#Format Bigrams and Print Sample of Extracted Vocabulary \n",
    "\n",
    "# The learned vocabulary can be retrieved from tfidf as a list using .get_feature_names_out()\n",
    "# Store the extracted vocabulary as vocab\n",
    "vocab = tfidf.get_feature_names_out()\n",
    "\n",
    "# Sklearn joins bigrams with a space character.\n",
    "# To make our output easier to read, replace the spaces in each term in vocab (a list of strings) with an underscore.\n",
    "vocab = [term.replace(' ', '_') for term in vocab]\n",
    "\n",
    "# Print the last 5 terms in the vocab\n",
    "#  The first term printed should be 'zipper_pocket'\n",
    "print(vocab[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform product descriptions into topics and print sample terms from topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 20)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform Topic Modeling with LDA\n",
    "\n",
    "# Now that we have our vectorized data, we can use Latent Direchlet Allocation to learn per-document topic distributions and per-topic term distributions.\n",
    "# Though the documents are likely composed of more, we'll model our dataset using 20 topics for ease of printing.\n",
    "\n",
    "# Import LatentDirichletAllocation from sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Instantiate a LatentDirichletAllocation model\n",
    "lda = LatentDirichletAllocation(n_components=20, random_state=512, n_jobs=-1)\n",
    "\n",
    "# Run fit_transform on lda using X_tfidf.\n",
    "X_lda = lda.fit_transform(X_tfidf)\n",
    "X_lda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_0 = [0.01 0.76 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n",
      " 0.14 0.01 0.01 0.01 0.01 0.01]\n",
      "\n",
      "n_topics_assigned_0 = 2\n",
      "\n",
      "assigned_topics_0 = [ 1 14]\n"
     ]
    }
   ],
   "source": [
    "# Get Assigned Topics for Product at df_jcp row 0\n",
    "\n",
    "# Get the assigned topic proportions for the document at row 0 of X_lda\n",
    "theta_0 = np.round(X_lda[0], 2)\n",
    "print(f'{theta_0 = :}\\n')\n",
    "\n",
    "# LDA will assign a small weight (or proability) to each topic for a document\n",
    "n_topics_assigned_0 = sum(theta_0 > 0.01)\n",
    "print(f'{n_topics_assigned_0 = :}\\n')\n",
    "\n",
    "# Indices of the assigned topics, sorted descending by the values in theta_0\n",
    "assigned_topics_0 = np.argsort(theta_0)[::-1][:n_topics_assigned_0]\n",
    "print(f'{assigned_topics_0 = :}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic # 0 : upper sole rubber rubber_sole synthetic\n",
      "Topic # 1 : dial case watch strap bracelet\n",
      "Topic # 2 : inseam waist zip pants shorts\n",
      "Topic # 3 : trunks side_pocket swim_trunks tongue_and take_your\n",
      "Topic # 4 : sleeveless machine_wash dress machine shoulder\n",
      "Topic # 5 : star_wars wars 16_piece photo star\n",
      "Topic # 6 : safe dishwasher dishwasher_safe set glass\n",
      "Topic # 7 : must_be garment must be_returned returned\n",
      "Topic # 8 : king comforter wipe set shams\n",
      "Topic # 9 : sold what clean_imported rod skin\n",
      "Topic #10 : nylon nylon_spandex bra straps hand_wash\n",
      "Topic #11 : ci bamboo count thread_count cotton_cover\n",
      "Topic #12 : short shirt collar short_sleeves cotton_washable\n",
      "Topic #13 : sterling jewelry_photos silver may stones\n",
      "Topic #14 : steel stainless stainless_steel cooking large\n",
      "Topic #15 : leather split coil mattress grain\n",
      "Topic #16 : moisture wicking moisture_wicking fabric tee\n",
      "Topic #17 : tone gold_tone silver_tone tone_metal metal_gold\n",
      "Topic #18 : resistant rug slip synthetic yes\n",
      "Topic #19 : add flair tank_top elastane flair_to\n"
     ]
    }
   ],
   "source": [
    "#Print Top Topic Terms\n",
    "\n",
    "# To get a sense of what each topic is composed of, we can print the most likely terms for each topic.\n",
    "\n",
    "# To make indexing easier, first convert vocab from a list to np.array()\n",
    "vocab = np.array(vocab)\n",
    "\n",
    "# assert that vocab is the correct datatype\n",
    "assert type(vocab) is np.ndarray, \"vocab needs to be converted to a numpy array\"\n",
    "\n",
    "# For each topic print f'Topic #{topic_idx:2d} : ' followed by the top 5 most likely terms in that topic.\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_terms_idx = topic.argsort()[-5:][::-1]\n",
    "    top_terms = vocab[top_terms_idx]\n",
    "    print(f'Topic #{topic_idx:2d} : {\" \".join(top_terms)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the description column of row 0, the assigned_topics_0 and \n",
    "# the top terms per topic above, our LDA model seems to have generated\n",
    "# topics that make sense given descriptions of department store goods, \n",
    "# with some a better fit than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate recommendations using topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 5000)\n"
     ]
    }
   ],
   "source": [
    "#  Generate Similarity Matrix\n",
    "\n",
    "# We'll use Content-Based Filtering to make recommendations based on a query product.\n",
    "\n",
    "# Import cosine_distances (not cosine_similarity) from sklearn.metrics.pairwise\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "# Use cosine_distances to generate similarity scores on our X_lda data\n",
    "distances = cosine_distances(X_lda)\n",
    "print(distances.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Invicta® Sl Rally Mens Black Leather Strap Chronograph Watch 16012'\n",
      " 'Bulova® Mens Black & Rose Gold-Tone Chronograph Sport Watch 98B104'\n",
      " 'Armitron® Now® Womens Two-Tone Crisscross Bangle Watch'\n",
      " 'Elgin® Mens Gold-Tone Skeleton Watch'\n",
      " 'Marvel Spiderman Tween Black Leather Strap Watch'\n",
      " 'Womens Nylon Strap Digital Sport Watch'\n",
      " 'Timex® Easy Reader Womens Black Strap Watch 2H341'\n",
      " 'Disney Womens Snow White Rose-Tone Black Enamel Watch'\n",
      " 'Citizen® Eco-Drive® Womens Stainless Steel Watch EW1250-54A'\n",
      " 'Zunammy® Mens Black Silicone Strap Sport Watch']\n"
     ]
    }
   ],
   "source": [
    "# Find Recommended Products\n",
    "\n",
    "# Let's test our proposed recommendation engine using the product at row 0 in df_jcp.\n",
    "\n",
    "# Print the names for the top 10 most similar products to this query.\n",
    "similar = np.argsort(distances[0])\n",
    "names = df_jcp['name_title'].values[similar[:10]]\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Sentiment Analysis Using Pipelines\n",
    "\n",
    "Here we will train a model to classify positive vs negative sentiment on a set of pet supply product reviews using sklearn Pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   review  10000 non-null  object\n",
      " 1   rating  10000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 156.4+ KB\n",
      "\n",
      "My cats are considerably more happy with this toy...and I don't have to leave the sofa to use it, given the long wand length. yay laziness!!\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "#  Load the Data\n",
    "\n",
    "# The dataset we'll be working with is a set of product reviews\n",
    "#   of pet supply items on Amazon.\n",
    "# This data is taken from https://nijianmo.github.io/amazon/index.html\n",
    "#   \"Justifying recommendations using distantly-labeled reviews and fined-grained aspects\"\n",
    "#   Jianmo Ni, Jiacheng Li, Julian McAuley\n",
    "#   Empirical Methods in Natural Language Processing (EMNLP), 2019\n",
    "\n",
    "# Load product reviews from ../data/amazon-petsupply-reviews_subset.csv.zips\n",
    "# Use pandas read_csv function with the default parameters as in part 1.\n",
    "# Store the resulting dataframe as df_amzn.\n",
    "df_amzn = pd.read_csv('../data/amazon-petsupply-reviews_subset.csv.zip')\n",
    "\n",
    "\n",
    "# print a summary of df_amzn using .info()\n",
    "# there should be 10000 rows with 2 columns\n",
    "df_amzn.info()\n",
    "\n",
    "# print blank line\n",
    "print() \n",
    "\n",
    "# print the review in the first row of the dataframe as an example\n",
    "print(df_amzn['review'][0])\n",
    "\n",
    "# print the rating in the first row of the dataframe as an example\n",
    "print(df_amzn['rating'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5    0.66\n",
      "4    0.14\n",
      "3    0.09\n",
      "1    0.06\n",
      "2    0.05\n",
      "Name: rating, dtype: float64\n",
      "\n",
      "True     0.66\n",
      "False    0.34\n",
      "Name: rating, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Transform Target\n",
    "\n",
    "# The ratings are originally in a 5 point scale\n",
    "# We'll turn this into a binary classification task to approximate positive vs negative sentiment\n",
    "\n",
    "# Print the proportions of values seen in the rating column\n",
    "print(df_amzn['rating'].value_counts(normalize=True).round(2))\n",
    "\n",
    "# Create a new binary target by setting\n",
    "#  rows where rating is 5 to True\n",
    "#  rows where rating is not 5 to False\n",
    "# Store in y\n",
    "y = df_amzn['rating'] == 5\n",
    "\n",
    "# print a blank line\n",
    "print()\n",
    "\n",
    "# Print the proportions of values seen in y\n",
    "#  using value_counts() with normalize=True\n",
    "# True here means a rating of 5 (eg positive)\n",
    "# False means a rating less than 5 (eg negative)\n",
    "print(y.value_counts(normalize=True).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True     0.66\n",
      "False    0.34\n",
      "Name: rating, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Train-test split\n",
    "\n",
    "# Import train_test_split from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split df_amzn.review and y into a train and test set\n",
    "# Store as reviews_train,reviews_test,y_train,y_test\n",
    "reviews_train, reviews_test, y_train, y_test = train_test_split(\n",
    "    df_amzn['review'], y, test_size=0.2, stratify=y, random_state=512\n",
    ")\n",
    "\n",
    "\n",
    "# print the proportion of values seen in y_train\n",
    "#  to confirm that the class distributions are the same\n",
    "print(y_train.value_counts(normalize=True).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tfidf', TfidfVectorizer(max_df=0.5, min_df=5)),\n",
      "                ('gbc', GradientBoostingClassifier(n_estimators=20))])\n"
     ]
    }
   ],
   "source": [
    "# Create a Pipeline of TfIdf transformation and Classification\n",
    "\n",
    "# import Pipeline and GradientBoostingClassifier from sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Create a pipeline with two steps: \n",
    "#  TfIdfVectorizer with min_df=5 and max_df=.5 named 'tfidf'\n",
    "#  GradientBoostingClassifier with 20 trees named 'gbc'\n",
    "pipe_gbc = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(min_df=5, max_df=0.5)),\n",
    "    ('gbc', GradientBoostingClassifier(n_estimators=20))\n",
    "])\n",
    "\n",
    "# Print out the pipeline\n",
    "# You should see both steps: tfidf and gbc\n",
    "print(pipe_gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gbc__max_depth': 10, 'tfidf__ngram_range': (1, 2)}\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "#  Perform Grid Search on pipe_gbc\n",
    "\n",
    "# import GridSearchCV from sklearn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a parameter grid to test using:\n",
    "#   unigrams or unigrams + bigrams in the tfidf step\n",
    "#   max_depth of 2 or 10 in the gbc step\n",
    "param_grid = {\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "    'gbc__max_depth': [2, 10]\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV to evaluate pipe_gbc on the values in param_grid\n",
    "#   use cv=2 and n_jobs=-1 to reduce run time\n",
    "# Fit on the training set of reviews_train,y_train\n",
    "# Store as gs_pipe_gbc\n",
    "gs_pipe_gbc = GridSearchCV(pipe_gbc, param_grid, cv=2, n_jobs=-1)\n",
    "gs_pipe_gbc.fit(reviews_train, y_train)\n",
    "\n",
    "# Print the best parameter settings in gs_pipe_gbc found by grid search\n",
    "print(gs_pipe_gbc.best_params_)\n",
    "\n",
    "# Print the best cv score found by grid search, with a precision of 2\n",
    "print(round(gs_pipe_gbc.best_score_, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76\n"
     ]
    }
   ],
   "source": [
    "#Evaluate on the test set\n",
    "\n",
    "# Calculate the test set (reviews_test,y_test) score using the trained gs_pipe_gbc \n",
    "test_score = gs_pipe_gbc.score(reviews_test, y_test)\n",
    "print(round(test_score, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on example reviews\n",
    "\n",
    "# Generate predictions for these two sentences using the fit gs_pipe_gbc:\n",
    "predictions = gs_pipe_gbc.predict([\n",
    "    'This is a great product.',\n",
    "    'This product is not great.'\n",
    "])\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eods-f23",
   "language": "python",
   "name": "eods-f23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
